{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pieQOy1UThI"
      },
      "source": [
        "# Classifica√ß√£o com BERT\n",
        "\n",
        "Neste *notebook* voc√™ ver√° um exemplo de como usar o [BERT](https://arxiv.org/abs/1810.04805) para tarefa de classifica√ß√£o, usando a bilbioteca **Transformers** do **Hugging Faces**. \n",
        "\n",
        "Fontes:  \n",
        "\n",
        "- [BERT Fine-Tuning Tutorial with PyTorch](https://mccormickml.com/2019/07/22/BERT-fine-tuning/).\n",
        "- [Hugging Faces - Github](https://github.com/huggingface/transformers) e [Hugging Faces - site](https://huggingface.co/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bq0Z5iWeUThO"
      },
      "source": [
        "Primeiro, vamos verificar se temos GPU dispon√≠vel para nossa execu√ß√£o. N√£o se preocupe caso n√£o possua GPU, apenas o treinamento ser√° mais demorado.\n",
        "\n",
        "Caso voc√™ esteja executando no Colab, acesse: Edit ü°í Notebook Settings ü°í Hardware accelerator ü°í (GPU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install accelerate\n",
        "!pip install bitsandbytes\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iH3iyyMUThO",
        "outputId": "86b2e17d-168a-4d63-c5c9-bd37b306740a",
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Verificar se tem GPU dispon√≠vel\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Informa PyTorch para usar GPU\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('Existe(m)  %d GPU(s) dispon√≠vel(eis).' % torch.cuda.device_count())\n",
        "\n",
        "    print('Vamos usar a GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# Se n√£o tem...\n",
        "else:\n",
        "    print('Sem GPU dispon√≠vel, usando CPU.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6riA3KZUThQ"
      },
      "source": [
        "### Biblioteca Transformers com *Pytorch*\n",
        "\n",
        "Aqui vamos usar a interface *Pytorch* porque possui um bom equil√≠brio entre as APIs de alto n√≠vel (f√°ceis de usar, mas sem fornecer informa√ß√µes sobre como as coisas funcionam) e c√≥digo de tensorflow (que cont√©m muitos detalhes, mas n√£o t√£o f√°ceis de usar).\n",
        "\n",
        "No momento, a biblioteca **Hugging Face** √© a interface *Pytorch* mais utilizada para trabalhar com BERT. Al√©m de oferecer suporte a uma variedade de modelos pr√©-treinados, tamb√©m inclui modelos especializados para tarefas espec√≠ficas. Neste tutorial vamos usar *BertForSequenceClassification*.\n",
        "\n",
        "A biblioteca tamb√©m inclui classes espec√≠ficas de tarefas para classifica√ß√£o de *tokens*, resposta a perguntas (Q&A), previs√£o da pr√≥xima frase, etc. O uso dessas classes pr√©-constru√≠das simplifica o processo de modifica√ß√£o de BERT para nossos prop√≥sitos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6KkxO5mUThR",
        "pycharm": {
          "is_executing": false,
          "name": "#%% md\n"
        }
      },
      "source": [
        "### An√°lise\n",
        "\n",
        "Podemos ver pelos nomes dos arquivos que temos as vers√µes originais e as tokenizadas dos dados.\n",
        "\n",
        "N√£o vamos usar a vers√£o pr√©-tokenizada porque, para aplicar o nosso modelo BERT pr√©-treinado, devemos usar o tokenizador fornecido pelo modelo. Isso porque: (1) o modelo tem um vocabul√°rio espec√≠fico e fixo e (2) o tokenizador BERT tem uma maneira particular de lidar com palavras fora do vocabul√°rio.\n",
        "\n",
        "Usaremos a biblioteca *pandas* para analisar o conjunto de treinamento e examinar algumas de suas propriedades."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "BrpMue0gUThR",
        "outputId": "7f62ff4b-9e81-4f2a-b627-34b9e2168999",
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Carrega o dataset em um dataframe pandas\n",
        "df = pd.read_csv(\"../dataset/cola_public/raw/in_domain_train.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Imprime o n√∫mero de frases.\n",
        "print('N√∫mero de senten√ßas de treinamento: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Exibe 10 frases rand√¥micas do dataset\n",
        "df.sample(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPl1u-8BUThS",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "As duas propriedades importantes s√£o as frases (*sentence*) e seus r√≥tulos (*label*), que podem ser 0 = incorreta e 1 = correta.\n",
        "\n",
        "Aqui est√£o cinco frases que s√£o rotuladas como incorretas gramaticalmente (em ingl√™s). Observe como essa tarefa √© muito mais dif√≠cil do que algo como an√°lise de sentimento!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "J4rp0KUmUThS",
        "outputId": "d2fc88ce-5eed-4663-e831-65083fbabbee"
      },
      "outputs": [],
      "source": [
        "df.loc[df.label == 0].sample(5)[['sentence', 'label']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPOPhM5uUThT"
      },
      "source": [
        "Vamos extrair as frases e seus r√≥tulos do nosso corpus de treinamento como *numpy ndarrays*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ozgqh2jUUThT",
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Lista de senten√ßas e seus r√≥tulos\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZrtVcKtUThT",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### Tokeniza√ß√£o\n",
        "\n",
        "Agora vamos transformar nosso conjunto de dados no formato em que o BERT pode ser treinado.\n",
        "\n",
        "Para alimentar o BERT com nosso texto, ele deve ser dividido em *tokens* e, em seguida, esses *tokens* devem ser mapeados para seu √≠ndice no vocabul√°rio do tokenizador.\n",
        "\n",
        "A tokeniza√ß√£o deve ser realizada pelo tokenizador inclu√≠do no modelo BERT que estamos trabalhando, que vamos baixar no c√≥digo abaixo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "0cb9049145974d55b3e8c9b075c16185",
            "e6bc4fa3801b45e189c6e7f3de641c06",
            "4f3b06cac2094a61b00f9a13b55bfc1c",
            "d00d3f968679442d9f69ba775ae843d1",
            "38e4081b0ee2413a9b525a0fef919d89",
            "6787ebe6ab364f83b72b90406adc6589",
            "7a6bcf541fe14666ae9996eee77f5205",
            "c2c278cfcaca4e718bcbd866769e1842"
          ]
        },
        "id": "N7x158YBUThT",
        "outputId": "7313952b-f603-4e0a-9a70-3fd3adf86b32",
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Carregar o tokenizador BERT.\n",
        "print('Carregando tokenizador BERT...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vH1IYgJnUThU",
        "pycharm": {
          "is_executing": false,
          "name": "#%% md\n"
        }
      },
      "source": [
        "Vamos aplicar o tokenizador a uma frase para ver a sa√≠da."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IF-KTEmyUThU",
        "outputId": "16b0c4c3-721b-471b-e3b6-3d3a43805b3d",
        "pycharm": {
          "is_executing": false,
          "name": "#%% \n"
        }
      },
      "outputs": [],
      "source": [
        "# Frase original\n",
        "print(' Original: ', sentences[0])\n",
        "\n",
        "# Frase dividida em tokens\n",
        "print('Tokenizada: ', tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "# Senten√ßa mapeada em token ids\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIaObrfhUThU",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Quando realmente convertermos todas as nossas senten√ßas, usaremos a fun√ß√£o ```tokenize.encode``` para lidar com ambas as etapas, em vez de chamar as fun√ß√µes que usamos acima separadamente.\n",
        "\n",
        "Antes de fazermos isso, precisamos falar sobre alguns dos requisitos de formata√ß√£o do BERT.\n",
        "\n",
        "### Formata√ß√£o\n",
        "\n",
        "O c√≥digo acima omitiu algumas etapas de formata√ß√£o necess√°rias que veremos aqui.\n",
        "\n",
        "Precisamos:\n",
        "1. Adicionar *tokens* especiais no in√≠cio e fim de cada frase.\n",
        "2. Prencher e truncar todas as frases para terem o mesmo comprimento.\n",
        "3. Diferenciar explicitamente *tokens* reais de *tokens* de preenchimento (PAD) com a ‚Äúm√°scara de aten√ß√£o‚Äù.\n",
        "\n",
        "**Tokens especiais**\n",
        "[SEP] - No final de cada frase, precisamos acrescentar o token especial [SEP], que √© usado em tarefas de duas senten√ßas, onde BERT recebe duas senten√ßas separadas e √© solicitado a determinar algo (por exemplo, a resposta √† pergunta na senten√ßa A pode ser encontrada na senten√ßa B).\n",
        "\n",
        "[CLS] - Para tarefas de classifica√ß√£o, devemos acrescentar o [CLS] ao in√≠cio de cada frase. Como o BERT consiste em 12 camadas de Tranformer, cada uma recebe uma lista de *embeddings* e produz o mesmo n√∫mero de *embeddings* na sa√≠da (com outros valores), na sa√≠da do transformador final (12¬∫), apenas o primeiro *embedding* (correspondente ao *token* [CLS]) √© usado pelo classificador .\n",
        "\n",
        "‚ÄúO primeiro *token* de cada sequ√™ncia √© sempre um *token* de classifica√ß√£o especial ([CLS]), onde o estado oculto final deste *token* √© usado como a representa√ß√£o da sequ√™ncia agregada para tarefas de classifica√ß√£o.‚Äù (do artigo BERT)\n",
        "\n",
        "N√£o precisamos pensar em estrat√©gias de *pool* sobre os *embeddings* finais, pois nesse token [CLS] de classifica√ß√£o, o modelo codificou tudo o que precisamos para a classifica√ß√£o naquele √∫nico vetor de incorpora√ß√£o de 768 valores. J√° est√° feito o *pool* para n√≥s!\n",
        "\n",
        "*Comprimento da frase e m√°scara de aten√ß√£o*\n",
        "\n",
        "As senten√ßas em nosso conjunto de dados obviamente t√™m comprimentos variados, ent√£o como o BERT lida com isso?\n",
        "\n",
        "BERT tem duas restri√ß√µes:\n",
        "\n",
        "- Todas as frases devem ser preenchidas ou truncadas em um √∫nico comprimento fixo.\n",
        "- O comprimento m√°ximo da frase √© 512 *tokens*.\n",
        "\n",
        "O preenchimento √© feito com um *token* especial [PAD], que est√° no √≠ndice 0 no vocabul√°rio BERT.\n",
        "\n",
        "A ‚ÄúM√°scara de Aten√ß√£o‚Äù √© simplesmente uma matriz de 1s e 0s indicando quais *tokens* s√£o de preenchimento e quais n√£o s√£o (parece meio redundante, n√£o √© ?). Essa m√°scara diz ao mecanismo de ‚ÄúAutoaten√ß√£o‚Äù do BERT para n√£o incorporar esses *tokens* (PAD) em sua interpreta√ß√£o da frase.\n",
        "\n",
        "O comprimento m√°ximo afeta a velocidade de treinamento e avalia√ß√£o. Por exemplo, com um Tesla K80:\n",
        "\n",
        "MAX_LEN = 128 --> Cada √©poca leva ~5:28 para treinar\n",
        "\n",
        "MAX_LEN = 64 --> Cada √©poca leva ~2:57 para treinar\n",
        "\n",
        "**Tokenizar o conjunto de dados**\n",
        "\n",
        "A biblioteca Transformers fornece uma fun√ß√£o ```encode``` que tratar√° da maioria das etapas de an√°lise e prepara√ß√£o de dados para n√≥s.\n",
        "\n",
        "Antes de estarmos prontos para codificar nosso texto, precisamos decidir sobre o comprimento m√°ximo de frase para preenchimento ou truncamento.\n",
        "\n",
        "A c√©lula abaixo realizar√° uma passagem de tokeniza√ß√£o em nosso conjunto de dados para medir o comprimento m√°ximo da frase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-bVl31sUThV",
        "outputId": "e1cc7fe2-8452-41b9-d5b2-142f20fae0d6",
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "max_len = 0\n",
        "\n",
        "# Para cada frase\n",
        "for sent in sentences:\n",
        "\n",
        "    # Tokeniza o texto e adiciona os tokens `[CLS]` e `[SEP]`\n",
        "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "\n",
        "    # Atualiza o comprimento m√°ximo da frase\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "\n",
        "print('Frase com tamanho m√°ximo no nosso dataset: ', max_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ph70NljUThW",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Para o caso de haver algumas senten√ßas mais longas no conjunto de teste, vams setar nosso comprimento m√°ximo em 64.\n",
        "\n",
        "Agora estamos prontos para realizar a tokeniza√ß√£o.\n",
        "\n",
        "A fun√ß√£o ```tokenizer.encode_plus``` realiza v√°rias etapas para n√≥s:\n",
        "\n",
        "1. Divide a frase em *tokens*.\n",
        "2. Adiciona os *tokens* especiais [CLS] e [SEP].\n",
        "3. Mapeia os *tokens* para seus IDs.\n",
        "4. Preenche ou trunca todas as frases com o mesmo comprimento.\n",
        "5. Cria as m√°scaras de aten√ß√£o que diferenciam explicitamente *tokens* reais de *tokens* [PAD].\n",
        "\n",
        "As primeiras quatro etapas s√£o realizadas pela fun√ß√£o ```tokenizer.encode```, mas vamos usar a ```tokenizer.encode_plus``` para a quinta etapa (m√°scaras de aten√ß√£o). [Documenta√ß√£o aqui](https://huggingface.co/transformers/main_classes/tokenizer.html?highlight=encode_plus#transformers.PreTrainedTokenizer.encode_plus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-99KsnSRUThW",
        "outputId": "3a90651d-b8ef-4d6d-e79c-b05723da4f23",
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Tokeniza todas as frases e mapeia os tokens em seus IDs\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# Para cada frase\n",
        "for sent in sentences:\n",
        "    # `encode_plus` far√° o seguinte:\n",
        "    #   (1) Tokeniza a frase\n",
        "    #   (2) Adiciona o token `[CLS]` no inicio\n",
        "    #   (3) Adiciona o token `[SEP]` no final\n",
        "    #   (4) Mapeia os tokens em seus IDs.\n",
        "    #   (5) Adiciona preenchimento (pad) ou trunca a frase at√° o comprimento m√°ximo (`max_length`)\n",
        "    #   (6) Cria m√°scara de aten√ß√£o para os tokens [PAD].\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Frase a ser codificada\n",
        "                        add_special_tokens = True, # Adiciona '[CLS]' e '[SEP]'\n",
        "                        max_length = 64,           # Preenche & trunca todas as frases\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Constr√≥i m√°scaras de aten√ß√£o\n",
        "                        return_tensors = 'pt',     # Returna tensores pytorch.\n",
        "                   )\n",
        "    \n",
        "    # Adiciona a frase codificada na lista\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # E sua m√°scara de aten√ß√£o (simplesmente diferencia tokens reais de tokens de preenchimento - PAD).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Converte a lista em tensores\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Imprime frase 0, agora como uma lista de IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpEzUITwUThX",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### Divis√£o dos dados - treinamento e valida√ß√£o\n",
        "\n",
        "Precisamos agora dividir nosso conjunto de treinamento, em 90% para treinamento e 10% para valida√ß√£o.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbziTD_xUThX",
        "outputId": "368c2ae2-bb8d-473b-dcde-26c42d5c9a22",
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "# Combina as entradas de treinamento em um TensorDataset.\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "# Criar uma divis√£o 90-10 para treinamento-valida√ß√£o.\n",
        "\n",
        "# Calcula o n√∫mero de inst√¢ncias para incluir em cada divis√£o\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "# Divide o dataset pegando randomicamente as inst√¢ncia \n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} inst√¢ncias de treinamento'.format(train_size))\n",
        "print('{:>5,} inst√¢ncias de valida√ß√£o'.format(val_size))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11MFCzUbUThY"
      },
      "source": [
        "Vamos criar um iterador para nosso conjunto de dados usando a classe *DataLoader*, para economizar mem√≥ria durante o treinamento, visto que, ao contr√°rio de um *loop* com *for*, com o iterador o conjunto de dados inteiro n√£o precisa ser carregado na mem√≥ria."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFdSvaIyUThY"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# O DataLoader precisa saber no tamanho do batch size para treinamento, ent√£o vamos espeficiar\n",
        "# Para o fine-tuning em uma tarefa, os autores recomendam um batch size de 16 ou 32.\n",
        "batch_size = 32\n",
        "\n",
        "# Criar os DataLoaders para nossos conjuntos de treinamento e teste\n",
        "# Vamos pegar inst√¢ncias de treinamento em ordem aleat√≥ria\n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # Exemplos de treinamento\n",
        "            sampler = RandomSampler(train_dataset), # Seleciona batches aleatoriamente\n",
        "            batch_size = batch_size # Treina com este batch size.\n",
        "        )\n",
        "\n",
        "# Para valida√ß√£o, a ordem n√£o importa, ent√£o vamos mant√™-la sequencial\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # Exemplos de valida√ß√£o.\n",
        "            sampler = SequentialSampler(val_dataset), # Executa os batches sequencialmente.\n",
        "            batch_size = batch_size # Avalia com este batch size.\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fF0XtfxIUThY"
      },
      "source": [
        "## Modelo de classifica√ß√£o\n",
        "\n",
        "Agora que nossos dados de entrada est√£o formatados corretamente, √© hora de especializar o modelo BERT.\n",
        "\n",
        "### BertForSequenceClassification\n",
        "\n",
        "Para a tarefa de classifica√ß√£o, vamos modificar o modelo BERT pr√©-treinado para fornecer sa√≠das para classifica√ß√£o e, em seguida, treinar todo modelo em nosso conjunto de dados at√© que esteja especializado para nossa tarefa.\n",
        "\n",
        "Felizmente, a implementa√ß√£o *Pytorch* do Hugging Face inclui um conjunto de interfaces projetadas para uma variedade de tarefas de PLN. Embora essas interfaces sejam todas constru√≠das em cima de um modelo BERT treinado, cada uma tem diferentes camadas superiores e tipos de sa√≠da projetados para nossa tarefa PLN espec√≠fica.\n",
        "\n",
        "Aqui est√° a lista atual de classes fornecidas para a especializa√ß√£o da tarefa (ajuste fino ou *fine tuning*):\n",
        "\n",
        "1. BertModel\n",
        "1. BertForPreTraining\n",
        "1. BertForMaskedLM\n",
        "1. BertForNextSentencePrediction\n",
        "1. BertForSequenceClassification (**usaremos aqui**)\n",
        "1. BertForTokenClassification\n",
        "1. BertForQuestionAnswering\n",
        "\n",
        "A documenta√ß√£o para eles pode ser encontrada [aqui](https://huggingface.co/transformers/v2.2.0/model_doc/bert.html).\n",
        "\n",
        "Estaremos usando *BertForSequenceClassification*, modelo BERT com uma camada linear √∫nica adicionada no topo para classifica√ß√£o, que ser√° usado como um classificador de frases. A medida que alimentamos os dados de entrada, todo o modelo BERT pr√©-treinado e a camada adicional de classifica√ß√£o n√£o treinada ser√£o treinados em nossa tarefa espec√≠fica.\n",
        "\n",
        "OK, vamos carregar o BERT! Existem alguns modelos diferentes de BERT pr√©-treinados dispon√≠veis. ‚Äúbert-base-uncased‚Äù, modell em ingl√™s, significa a vers√£o que tem apenas letras min√∫sculas (‚Äúsem caixa‚Äù) e √© a vers√£o menor das duas (‚Äúbase‚Äù vs ‚Äúlarge‚Äù).\n",
        "\n",
        "A documenta√ß√£o do m√©todo ```from_pretrained``` pode ser encontrada [aqui](https://huggingface.co/transformers/v2.2.0/main_classes/model.html#transformers.PreTrainedModel.from_pretrained), com os par√¢metros adicionais definidos [aqui](https://huggingface.co/transformers/v2.2.0/main_classes/configuration.html#transformers.PretrainedConfig)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZ1XxRwlUThY",
        "outputId": "9534de38-4405-4b4e-da69-012c118a33ee"
      },
      "outputs": [],
      "source": [
        "from transformers import BertForSequenceClassification, BertConfig\n",
        "from torch.optim import AdamW\n",
        "\n",
        "# Carrega a classe BertForSequenceClassification, o modelo pre-treinado com uma camada linear simples no topo\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Usa o BERT com 12 camadas, com vocabulario com caixa baixa\n",
        "    num_labels = 2, # O n√∫mero de sa√≠das, ou r√≥tulos, do nosso modelo (classifica√ß√£o bin√°ria em nosso caso)\n",
        "                    # Para tarefas com mais classes (multi-classe), podemos aumentar esse n√∫mero\n",
        "    output_attentions = False, # Se o modelo deve retornar os pesos de aten√ß√£o\n",
        "    output_hidden_states = False, # Se o modelo deve retornar todos estados escondidos\n",
        ")\n",
        "\n",
        "# Se voc√™ tem GPU, configura o pytorch para executar o modelo na GPU\n",
        "model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGO0u1gjUThZ"
      },
      "source": [
        "Apenas por curiosidade, podemos navegar por todos os par√¢metros do modelo.\n",
        "\n",
        "Na c√©lula abaixo, imprimimos os nomes e dimens√µes dos pesos para:\n",
        "\n",
        "1. A camada de incorpora√ß√£o.\n",
        "1. O primeiro dos doze transformadores.\n",
        "1. A camada de sa√≠da."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xnrLqqkUThZ",
        "outputId": "9aafea18-6c67-42d6-9856-2f07f903e84f"
      },
      "outputs": [],
      "source": [
        "# Lista todos os paremtros do modelo como uma lista de tuplas\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('O modelo BERT tem {:} par√¢metros diferentes.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Camada de incorpora√ß√£o (Embedding) ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Primeiro Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Camada de Sa√≠da ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hBWGUmgUThZ"
      },
      "source": [
        "## Otimizador e taxas de aprendizagem ##\n",
        "\n",
        "Agora que carregamos nosso modelo, precisamos pegar os hiperpar√¢metros de treinamento de dentro do modelo armazenado.\n",
        "\n",
        "Para fins de ajuste fino, os autores recomendam escolher entre os seguintes valores (do Ap√™ndice A.3 do artigo de BERT ):\n",
        "\n",
        "1. Tamanho do lote (*batch size*): 16, 32\n",
        "1. Taxa de aprendizagem (*Adam*): 5e-5, 3e-5, 2e-5\n",
        "1. N√∫mero de √©pocas: 2, 3, 4\n",
        "\n",
        "Aqui vamos usar:\n",
        "\n",
        "1. Tamanho do lote: 32 (definido ao criar nossos *DataLoaders*)\n",
        "1. Taxa de aprendizagem: 2e-5\n",
        "1. √âpocas: 4 (veremos que provavelmente s√£o muitas ...)\n",
        "\n",
        "O par√¢metro ```eps = 1e-8``` √© ‚Äúum n√∫mero muito pequeno para evitar qualquer divis√£o por zero na implementa√ß√£o‚Äù (leia mais [aqui](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)).\n",
        "\n",
        "Voc√™ pode encontrar a cria√ß√£o do otimizador *AdamW* em ```run_glue.py``` [aqui](https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L109)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kH48Pnl1UThZ"
      },
      "outputs": [],
      "source": [
        "# Nota: AdamW √© uma classe da biblioteca huggingface\n",
        "# Provavelmente o 'W' √© de 'Weight Decay fix\" ('Corre√ß√£o de redu√ß√£o de peso \")\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - o padr√£o √©  5e-5, aqui usamos 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - o padr√£o √© 1e-8.\n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Yk6BlpEUThZ"
      },
      "outputs": [],
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# N√∫mero de √©pocas de treinamento. Os autores do BERT recomendam entre 2 a 4. \n",
        "# N√≥s escolhemos 4, mas vamos ver depois que isso pode estar causando overfit nos dados de treinamento\n",
        "epochs = 4\n",
        "\n",
        "# N√∫mero total de passos de treinamento √© [n√∫mero de batches] x [n√∫mero de √©pocas]. \n",
        "# (Note que n√£o √© o mesmo que o n√∫mero de inst√¢ncias de treinamento).\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Criando o programador de taxa de aprendizagem (learning rate scheduler)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Valor padr√£o no run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xK0zVkdGUTha"
      },
      "source": [
        "## *Loop* de treinamento ##\n",
        "\n",
        "Abaixo est√° o nosso *loop* de treinamento. H√° muita coisa acontecendo, mas fundamentalmente para cada passagem em nosso *loop*, temos uma fase de treinamento e uma fase de valida√ß√£o.\n",
        "\n",
        "**Treinamento:**\n",
        "\n",
        "1. Descompacta nossas entradas (dados e r√≥tulos)\n",
        "1. Carrega dados na GPU para acelera√ß√£o (quando necess√°rio)\n",
        "1. Limpa os gradientes calculados na passagem anterior\n",
        "(no *pytorch*, os gradientes se acumulam por padr√£o (√∫til para RNNs), a menos que a gente limpe explicitamente)\n",
        "1. Passo *Forward* (avan√ßo), alimentando os dados de entrada pela rede\n",
        "1. Passo *Backward* (para tr√°s), ou retropropaga√ß√£o\n",
        "1. Informa a rede para atualizar os par√¢metros com ```optimizer.step ()```\n",
        "1. Rastreia vari√°veis para monitorar o progresso\n",
        "\n",
        "**Avalia√ß√£o:**\n",
        "\n",
        "1. Descompacta nossas entradas (dados e r√≥tulos)\n",
        "1. Carrega dados na GPU para acelera√ß√£o (quando necess√°rio)\n",
        "1. Passo *Forward* (avan√ßo), alimentando os dados de entrada pela rede\n",
        "1. Calcula a perda (*loss*) em nossos dados de valida√ß√£o e rastreia vari√°veis para monitorar o progresso\n",
        "\n",
        "*Pytorch* esconde todos os c√°lculos detalhados, mas comentamos o c√≥digo para apontar quais das etapas acima est√£o acontecendo em cada linha.\n",
        "\n",
        "No c√≥digo abaixo, definimos uma fun√ß√£o auxiliar para calcular a precis√£o."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSljX7FAUTha"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Fun√ß√£o para caluclar a acur√°cia das nossas perdi√ß√µes x r√≥tulos\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDjNSgJjUTha"
      },
      "source": [
        "Fun√ß√£o auxiliar para formatar o tempo decorrido (hh:mm:ss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEsSXtJCUTha"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Pega o tempo em segundos e retorna como hh:mm:ss\n",
        "    '''\n",
        "    # Arredonda\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Formata como hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9Rur-k-UTha"
      },
      "source": [
        "Estamos prontos para come√ßar o treinamento!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uzk4DAAgUThb",
        "outputId": "014c103e-3598-48f4-b0be-71b427941102"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import time\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Definir fun√ß√£o para formatar o tempo\n",
        "def format_time(seconds):\n",
        "    return \"{:0>2}:{:0>2}:{:05.2f}\".format(int(seconds//3600), int((seconds%3600)//60), seconds%60)\n",
        "\n",
        "# Configurar semente para reprodutibilidade\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Verificar se h√° GPU dispon√≠vel\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Inicializar estat√≠sticas de treinamento\n",
        "training_stats = []\n",
        "total_t0 = time.time()\n",
        "\n",
        "# Fun√ß√£o para calcular acur√°cia\n",
        "def flat_accuracy(logits, labels):\n",
        "    preds = np.argmax(logits, axis=1).flatten()\n",
        "    labels = labels.flatten()\n",
        "    return np.sum(preds == labels) / len(labels)\n",
        "\n",
        "# Loop de treinamento\n",
        "for epoch_i in range(0, epochs):\n",
        "    print(f\"\\n======== √âpoca {epoch_i + 1} / {epochs} ========\")\n",
        "    print(\"Treinando...\")\n",
        "    \n",
        "    t0 = time.time()\n",
        "    total_train_loss = 0\n",
        "    model.train()\n",
        "    \n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print(f'  Batch {step:>5,} de {len(train_dataloader):>5,}. Tempo: {elapsed}')\n",
        "        \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        model.zero_grad()\n",
        "        \n",
        "        # Chamada atualizada para vers√µes recentes do Transformers\n",
        "        outputs = model(b_input_ids, \n",
        "                       token_type_ids=None, \n",
        "                       attention_mask=b_input_mask, \n",
        "                       labels=b_labels)\n",
        "        \n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "        \n",
        "        total_train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "    \n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "    training_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(f\"\\n  Perda m√©dia do treinamento: {avg_train_loss:.2f}\")\n",
        "    print(f\"  √âpoca de treinamento levou: {training_time}\")\n",
        "    \n",
        "    # Valida√ß√£o\n",
        "    print(\"\\nExecutando Valida√ß√£o...\")\n",
        "    t0 = time.time()\n",
        "    \n",
        "    model.eval()\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    \n",
        "    for batch in validation_dataloader:\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = model(b_input_ids, \n",
        "                          token_type_ids=None, \n",
        "                          attention_mask=b_input_mask,\n",
        "                          labels=b_labels)\n",
        "        \n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "        \n",
        "        total_eval_loss += loss.item()\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "    \n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(f\"  Acur√°cia: {avg_val_accuracy:.2f}\")\n",
        "    print(f\"  Valida√ß√£o Perda (Loss): {avg_val_loss:.2f}\")\n",
        "    print(f\"  Valida√ß√£o levou: {validation_time}\")\n",
        "    \n",
        "    training_stats.append({\n",
        "        'epoch': epoch_i + 1,\n",
        "        'Training Loss': avg_train_loss,\n",
        "        'Valid. Loss': avg_val_loss,\n",
        "        'Valid. Accur.': avg_val_accuracy,\n",
        "        'Training Time': training_time,\n",
        "        'Validation Time': validation_time\n",
        "    })\n",
        "\n",
        "print(\"\\nTreinamento completo!\")\n",
        "print(f\"Tempo total de treinamento {format_time(time.time()-total_t0)} (h:mm:ss)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vyJO3kOUThb"
      },
      "source": [
        "Vamos ver o resumo do processo de treinamento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "IQYHD9T8UThb",
        "outputId": "f17399fd-57cf-4238-a5a3-940c03bdbcf3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Mostra n√∫meros com duas casas decimais\n",
        "pd.set_option('display.precision', 2)\n",
        "\n",
        "# Cria um DataFrame das nossas estat√≠sticas de treinamento\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "\n",
        "# Usa a √©poca como o √≠ndice da linha\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "# For√ßar o agrupamento dos cabe√ßalho da coluna \n",
        "# df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
        "\n",
        "# Mostra a tabela\n",
        "df_stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guNi39XGUThb"
      },
      "source": [
        "Observe que, enquanto a perda de treinamento est√° diminuindo a cada √©poca, a perda de valida√ß√£o est√° aumentando! Isso sugere que estamos treinando nosso modelo por muito tempo e que ele est√° se ajustando demais aos dados de treinamento.\n",
        "\n",
        "(Para refer√™ncia, estamos usando 7.695 amostras de treinamento e 856 amostras de valida√ß√£o).\n",
        "\n",
        "A perda de valida√ß√£o √© uma medida mais precisa do que a precis√£o, porque com a precis√£o n√£o nos importamos com o valor de sa√≠da exato, mas apenas em que lado de um limite ele cai.\n",
        "\n",
        "Se estivermos prevendo a resposta correta, mas com menos confian√ßa, a perda de valida√ß√£o pegar√° isso, mas a precis√£o n√£o."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install matplotlib\n",
        "!pip install seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "id": "J3OyoiaCUThb",
        "outputId": "8c2b788f-4c46-4192-f5c4-8eb09e602391"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Usando estilo\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Aumentando o tamanho e fonte \n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plotando a curva de aprendizagem\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Treinamento\")\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Valida√ß√£o\")\n",
        "\n",
        "# Adicionando t√≠tulos\n",
        "plt.title(\"Perda de treinamento e valida√ß√£o\")\n",
        "plt.xlabel(\"√âpoca\")\n",
        "plt.ylabel(\"Perda\")\n",
        "plt.legend()\n",
        "plt.xticks([1, 2, 3, 4])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjyahGzSUThc"
      },
      "source": [
        "## Desempenho no conjunto de teste \n",
        "\n",
        "Agora, carregaremos o conjunto de dados *holdout* e preparamos as entradas, como fizemos com o conjunto de treinamento. \n",
        "\n",
        "Em seguida, avaliaremos as previs√µes usando o [coeficiente de correla√ß√£o de Matthew](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html), m√©trica usada para avaliar o desempenho no CoLA. Com essa m√©trica, +1 √© a melhor pontua√ß√£o e -1 √© a pior pontua√ß√£o. Dessa forma, podemos ver nosso desempenho em rela√ß√£o aos modelos de √∫ltima gera√ß√£o para essa tarefa espec√≠fica.\n",
        "\n",
        "### Prepara√ß√£o de dados \n",
        "\n",
        "Precisaremos aplicar todas as mesmas etapas que fizemos para os dados de treinamento para preparar nosso conjunto de dados de teste.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4GZU-RGUThc",
        "outputId": "5a14d864-b278-4fa4-b24a-e8a120bbed00"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Carrega o dataset em um dataframe pandas\n",
        "df = pd.read_csv(\"../dataset/cola_public/raw/out_of_domain_dev.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Imprime o n√∫mero de frases\n",
        "print('N√∫mero de frases de teste: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Cria as listas de frases e r√≥tulos\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values\n",
        "\n",
        "# Tokeniza todas as senten√ßas e mapeia os tokens em seus IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# Para cada frase\n",
        "for sent in sentences:\n",
        "    # `encode_plus` vai:\n",
        "    #   (1) Tokenizar a frase\n",
        "    #   (2) Adicionar o token `[CLS]` no in√≠cio\n",
        "    #   (3) Adicionar o token `[SEP]` no final.\n",
        "    #   (4) Mapear tokens aos seus IDs.\n",
        "    #   (5) Preencher ou truncar a frase at√© `max_length`\n",
        "    #   (6) Criar m√°scara de aten√ß√£o para os tokens [PAD].\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Senten√ßa a ser codificada\n",
        "                        add_special_tokens = True, # Adiciona '[CLS]' e '[SEP]'\n",
        "                        max_length = 64,           # Preenche & trunca todas as senten√ßas\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Constr√≥i m√°scara de aten√ß√£o\n",
        "                        return_tensors = 'pt',     # Retorna tensores pytorch.\n",
        "                   )\n",
        "    \n",
        "    # Adiciona a senten√ßa codificada na lista\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # E sua m√°scara de aten√ß√£o (simplesmente diferencia preenchimento (PAD) de n√£o-pad).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Converte as listas em tensores.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Seta o batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Cria o DataLoader.\n",
        "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeqtTUuWUThc"
      },
      "source": [
        "### Avalia√ß√£o no conjunto de teste ###\n",
        "\n",
        "Com o conjunto de teste preparado, podemos aplicar nosso modelo ajustado para gerar previs√µes no conjunto de teste."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOO9WqCkUThc",
        "outputId": "ef6f533a-a422-4a03-daad-7e0ca2c5e1bf"
      },
      "outputs": [],
      "source": [
        "# Predi√ß√£o no conjunto de teste\n",
        "\n",
        "print('Predi√ß√£o de r√≥tulos para {:,} frases de teste...'.format(len(input_ids)))\n",
        "\n",
        "# coloca o modele em modo de avalia√ß√£o\n",
        "model.eval()\n",
        "\n",
        "# Vari√°veis de rastreamento\n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predi√ß√£o \n",
        "for batch in prediction_dataloader:\n",
        "  # Adiciona o batch na GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Retira as entradas do dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Informa o modelo para n√£o computar gradientes, salvando mem√≥ria e acelerando a predi√ß√£o \n",
        "  with torch.no_grad():\n",
        "      # Passo Forward, calcula as predi√ß√µes logit\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # Move as logits e r√≥tulos para CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Armazen predi√ß√µes e r√≥tulos verdadeiros\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('FIM.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Diwh1xgqUThc"
      },
      "source": [
        "A precis√£o no *benchmark* CoLA √© medida usando o ‚Äúcoeficiente de correla√ß√£o de Matthews‚Äù (MCC).\n",
        "\n",
        "Usamos MCC aqui porque as classes s√£o desequilibradas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ss3ueMk5UThd",
        "outputId": "6042df38-2cb2-4919-feba-f259fee43221"
      },
      "outputs": [],
      "source": [
        "print('Inst√¢ncias positivas: %d de %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install scikit-metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pP02zqm4UThd",
        "outputId": "60feb3eb-64bf-4c36-9cbe-fe1647079ba9"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "# Avaliando cada batch de teste usando MCC\n",
        "print('Calculando MCC para cada batch...')\n",
        "\n",
        "# Para cada batch\n",
        "for i in range(len(true_labels)):\n",
        "  \n",
        "  # As predi√ß√µes para este batch s√£o a 2a. coluna do ndarray (uma coluna para \"0\" e outra para \"1\")\n",
        "  # Pega o r√≥tulo com o maior valor e transforma em uma lista de 0s e 1s.\n",
        "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "  \n",
        "  # Calcula e armazena o coef para este batch.  \n",
        "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
        "  matthews_set.append(matthews)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNnOntTjUThd"
      },
      "source": [
        "A pontua√ß√£o final ser√° baseada em todo o conjunto de teste, mas vamos dar uma olhada nas pontua√ß√µes dos lotes individuais para ter uma no√ß√£o da variabilidade da m√©trica entre os lotes.\n",
        "\n",
        "Cada lote cont√©m 32 senten√ßas, exceto o √∫ltimo lote que cont√©m apenas (516% 32) = 4 senten√ßas de teste."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "2FdSg9ZVUThd",
        "outputId": "b0a57176-f9c9-45c0-fd91-db93b2bee650"
      },
      "outputs": [],
      "source": [
        "# Cria um barplot mostrando o MCC para cada batch dos nossos exemplos de teste.\n",
        "ax = sns.barplot(x=list(range(len(matthews_set))), y=matthews_set, ci=None)\n",
        "\n",
        "plt.title('MCC por Batch')\n",
        "plt.ylabel('MCC Score (-1 a +1)')\n",
        "plt.xlabel('Batch #')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIprQ7dIUThe"
      },
      "source": [
        "Agora combinaremos os resultados de todos os lotes e calcularemos nossa pontua√ß√£o final no MCC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgg1F-lCUThe",
        "outputId": "14162985-fa56-4e49-df3e-8e3aea3bb5a4"
      },
      "outputs": [],
      "source": [
        "# Combina os resultados para todos os batches. \n",
        "flat_predictions = np.concatenate(predictions, axis=0)\n",
        "\n",
        "# Para cada exemplo, pega o r√≥tulo (0 ou 1) com maior score\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combina os r√≥tulos para cada batch em uma √∫nica lista.\n",
        "flat_true_labels = np.concatenate(true_labels, axis=0)\n",
        "\n",
        "# Calcula o MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('Total MCC: %.3f' % mcc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8G2SYdJSUThe"
      },
      "source": [
        "Legal! Em cerca de meia hora e sem fazer nenhum ajuste de hiperpar√¢metro (ajustando a taxa de aprendizagem, √©pocas, tamanho do lote, propriedades ADAM, etc.), obtivemos uma boa pontua√ß√£o."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wbeNbEZUThe"
      },
      "source": [
        "Nota: Para maximizar a pontua√ß√£o, devemos remover o ‚Äúconjunto de valida√ß√£o‚Äù (que usamos para ajudar a determinar em quantas √©pocas treinar) e treinar em todo o conjunto de treinamento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AddBAXzdUThe"
      },
      "source": [
        "A biblioteca documenta a precis√£o esperada para este *benchmark* aqui como **49.23** ([ver tabela de classifica√ß√£o oficial](https://gluebenchmark.com/leaderboard/submission/zlssuBTm5XRs0aSKbFYGVIVdvbj1/-LhijX9VVmvJcvzKymxy)).\n",
        "\n",
        "## Conclus√£o \n",
        "\n",
        "Com um modelo BERT pr√©-treinado, podemos criar de forma r√°pida e eficaz um modelo de alta qualidade com o m√≠nimo de esfor√ßo e tempo de treinamento usando a interface *Pytorch*, independentemente da tarefa PLN espec√≠fica em que estamos interessados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2vM0vX2UThe"
      },
      "source": [
        "### Salvar e carregar o modelo ajustado \n",
        "\n",
        "A pr√≥xima c√©lula grava o modelo e o tokenizador no disco."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2clVnjW1UThf",
        "outputId": "2a2a0a6a-57e7-4d00-db46-1b5504de01f1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Boa-pr√°tica: usar os nomes padr√£o dos modelos, para fazer load usando from_pretrained()\n",
        "os.makedirs(\"../model\", exist_ok=True)\n",
        "output_dir = '../model/'\n",
        "\n",
        "# Criar diret√≥rio de sa√≠da se necess√°rio\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Savando modelo em %s\" % output_dir)\n",
        "\n",
        "# Salva um modelo treinado, sua configura√ß√£o e tokenizador com `save_pretrained()`.\n",
        "# Eles podem ser carregados com `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'bert_classifier') else model  # Cuida do treinamento paralelo/distribu√≠do\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Boa-pr√°tica: salve os argumentos de treinamento junto com o modelo\n",
        "# torch.save(args, os.path.join(output_dir, 'bert_classifier.bin'))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "jupyter-bert-classificacao.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0cb9049145974d55b3e8c9b075c16185": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4f3b06cac2094a61b00f9a13b55bfc1c",
              "IPY_MODEL_d00d3f968679442d9f69ba775ae843d1"
            ],
            "layout": "IPY_MODEL_e6bc4fa3801b45e189c6e7f3de641c06"
          }
        },
        "38e4081b0ee2413a9b525a0fef919d89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "4f3b06cac2094a61b00f9a13b55bfc1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6787ebe6ab364f83b72b90406adc6589",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_38e4081b0ee2413a9b525a0fef919d89",
            "value": 231508
          }
        },
        "6787ebe6ab364f83b72b90406adc6589": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a6bcf541fe14666ae9996eee77f5205": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c2c278cfcaca4e718bcbd866769e1842": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d00d3f968679442d9f69ba775ae843d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2c278cfcaca4e718bcbd866769e1842",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7a6bcf541fe14666ae9996eee77f5205",
            "value": " 232k/232k [00:00&lt;00:00, 939kB/s]"
          }
        },
        "e6bc4fa3801b45e189c6e7f3de641c06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
