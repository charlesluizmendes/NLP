{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00eb3585",
   "metadata": {},
   "source": [
    "# Classificação de texto com BERT\n",
    "\n",
    "## O que é classificação de texto?\n",
    "\n",
    "Classificação de texto é um subcampo de aprendizado de máquina que ensina computadores a classificar texto em diferentes categorias. É comumente usado como uma técnica de aprendizado supervisionado, o que significa que o algoritmo é treinado em um conjunto de textos que já foram rotulados com suas respectivas categorias. Uma vez treinado nesses dados, o algoritmo pode usar o que aprendeu para fazer previsões sobre novos textos não rotulados.\n",
    "\n",
    "O algoritmo procura padrões no texto para determinar a qual categoria ele pertence. É como quando aprendemos a reconhecer um certo tipo de flor — começamos a notar certas características que a distinguem de outros tipos de flores. Com a classificação de texto, o algoritmo está fazendo a mesma coisa, mas com palavras e frases.\n",
    "\n",
    "A classificação de texto é uma ferramenta versátil amplamente usada em muitas aplicações do mundo real que você pode ter encontrado. Por exemplo, um e-mail que acabou na sua pasta de spam é a classificação de texto em ação. O modelo pode diferenciar entre e-mails de spam e não spam estudando palavras ou frases específicas que identificam um determinado e-mail como spam, como \"Parabéns, você ganhou\" ou \"Hoje é seu dia de sorte\".\n",
    "\n",
    "A classificação de texto também é valiosa na análise do sentimento de postagens de mídia social, particularmente quando se trata de detectar sentimentos negativos como discurso de ódio. Ao usar um modelo de aprendizado de máquina, o texto pode ser classificado e monitorado para linguagem ofensiva e discurso de ódio.\n",
    "\n",
    "Mas a classificação de texto não serve apenas para aplicações sérias — ela também pode ser usada para coisas divertidas, como categorizar artigos de notícias e vídeos por tópico. Dessa forma, um usuário pode facilmente encontrar artigos e vídeos que lhe interessam sem ter que peneirar conteúdo irrelevante. A classificação de texto é realmente uma ferramenta poderosa com uma variedade de aplicações práticas.\n",
    "\n",
    "## O que é BERT para aprendizado profundo?\n",
    "\n",
    "BERT, abreviação de Bidirectional Encoder Representations from Transformers, é um poderoso modelo de processamento de linguagem natural (PLN) desenvolvido pelo Google que usa uma arquitetura de rede neural profunda baseada no modelo de transformador de última geração.\n",
    "\n",
    "Como dissemos antes, a arquitetura do modelo BERT é baseada em uma rede neural profunda chamada transformador, que é diferente dos modelos NLP tradicionais que processam texto uma palavra por vez. Em vez disso, os transformadores podem processar toda a entrada de texto de uma só vez, o que os ajuda a capturar as relações entre palavras e frases de forma mais eficaz.\n",
    "\n",
    "## Como o modelo BERT funciona para classificação de texto?\n",
    "\n",
    "O BERT usa um codificador transformador bidirecional multicamadas para representar o texto de entrada em um espaço de alta dimensão. Isso significa que ele pode levar em conta todo o contexto de cada palavra na frase, o que o ajuda a entender melhor o significado do texto.\n",
    "\n",
    "Uma das coisas mais interessantes sobre o BERT é que ele é um modelo pré-treinado. Isso significa que o BERT pode ser treinado em grandes quantidades de dados de texto, como livros, artigos e sites, antes de ser ajustado para tarefas específicas de PNL downstream, incluindo classificação de texto.\n",
    "\n",
    "Ao pré-treinar em um grande corpus de dados de texto, o BERT pode desenvolver uma compreensão profunda da estrutura e do significado subjacentes da linguagem, tornando-o uma ferramenta altamente eficaz para tarefas de PNL. Uma vez pré-treinado, o BERT pode ser ajustado para tarefas específicas, o que permite que ele se adapte às nuances específicas da tarefa e melhore sua precisão.\n",
    "\n",
    "É importante notar que há duas variações diferentes do BERT: BERT base e BERT grande. Para o restante deste artigo, utilizaremos o modelo base do BERT, que é uma versão mais compacta do BERT que ainda mantém uma forte compreensão do contexto e das nuances linguísticas. Ao empregar o BERT base, podemos aproveitar suas habilidades enquanto reduzimos as demandas computacionais, tornando-o mais adequado para uma variedade de tarefas de classificação de texto e alcançando resultados notáveis.\n",
    "\n",
    "## Tutorial sobre classificação de texto usando BERT\n",
    "\n",
    "Então por que ajustamos o BERT no conjunto de dados de resenhas de filmes do IMDB? Bem, queremos adaptar o já poderoso modelo BERT para tarefas de análise de sentimentos. O BERT é excelente para entender a estrutura e o contexto da linguagem, mas não tem habilidades naturais de análise de sentimentos. Ao ajustar o BERT para classificação de texto com um conjunto de dados rotulado, como resenhas de filmes do IMDB, damos a ele a capacidade de prever com precisão os sentimentos nas frases que ele encontra.\n",
    "\n",
    "Caso você queira executar o modelo abaixo, você pode encontrar o conjunto de dados IMDB no seguinte link:\n",
    "\n",
    "[Conjunto de dados do IMDB com 50 mil críticas de filmes](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3c5c77",
   "metadata": {},
   "source": [
    "## Etapa 1: Importe as bibliotecas necessárias\n",
    "\n",
    "Este trecho de código é sobre importar as ferramentas essenciais que precisamos para nosso projeto. Estamos usando PyTorch para a funcionalidade de aprendizado profundo, a biblioteca transformers para BERT e métodos essenciais da biblioteca scikit-learn para manipular dados e verificar o quão bem nosso modelo funciona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745a33fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install scikit-learn \n",
    "!pip install pandas\n",
    "!pip install accelerate\n",
    "!pip install bitsandbytes\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d16a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716c2091",
   "metadata": {},
   "source": [
    "## Etapa 2: Importe o conjunto de dados IMDB e pré-processe-o\n",
    "\n",
    "O código abaixo define uma função load_imdb_data que lê um arquivo CSV contendo resenhas de filmes do IMDB e seus sentimentos correspondentes. Ele retorna uma lista de textos de resenhas e uma lista de rótulos, onde 1 representa um sentimento positivo e 0 representa um sentimento negativo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8778125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imdb_data(data_file):\n",
    "    df = pd.read_csv(data_file)\n",
    "    texts = df['review'].tolist()\n",
    "    labels = [1 if sentiment == \"positive\" else 0 for sentiment in df['sentiment'].tolist()]\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f578935",
   "metadata": {},
   "source": [
    "Em seguida, salvaremos nosso diretório de conjunto de dados e o inseriremos como uma entrada na função load_imdb_data()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b214cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"../dataset/IMDB Dataset.csv\"\n",
    "texts, labels = load_imdb_data(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0934b827",
   "metadata": {},
   "source": [
    "## Etapa 3: Crie uma classe de conjunto de dados personalizada para classificação de texto\n",
    "\n",
    "Esta é uma classe de conjunto de dados personalizado que ajuda a organizar as avaliações de filmes e seus sentimentos para nosso modelo BERT. Ela cuida da tokenização do texto, manipulando o comprimento da sequência e fornecendo um pacote organizado com IDs de entrada, máscaras de atenção e rótulos para nosso modelo aprender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111f8b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(text, return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True)\n",
    "        return {'input_ids': encoding['input_ids'].flatten(), 'attention_mask': encoding['attention_mask'].flatten(), 'label': torch.tensor(label)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caf2b3a",
   "metadata": {},
   "source": [
    "## Etapa 4: construir nosso classificador BERT do cliente\n",
    "\n",
    "Nesta etapa, pretendemos criar nosso próprio classificador BERT personalizado. O classificador é construído sobre o famoso modelo BERT, que é ótimo para entender texto. Adicionaremos então uma camada de dropout para manter as coisas sob controle e uma camada linear para nos ajudar a classificar o texto.\n",
    "\n",
    "Nosso BERTClassifier pega alguns IDs de entrada e máscaras de atenção, e os executa por meio do BERT e das camadas extras que adicionamos. O classificador retorna nossa saída como pontuações de classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae9d7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, bert_model_name, num_classes):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        x = self.dropout(pooled_output)\n",
    "        logits = self.fc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba23c8e",
   "metadata": {},
   "source": [
    "## Etapa 5: Defina a função train()\n",
    "\n",
    "A função train() pega o modelo, o carregador de dados, o otimizador, o planejador e o dispositivo como seus trainees. A função coloca o modelo no modo de treinamento e então executa cada lote de dados do carregador de dados. Para cada lote, ela limpa os gradientes do otimizador, obtém os IDs de entrada, máscaras de atenção e rótulos, e os alimenta para o modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8b6856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3e1241",
   "metadata": {},
   "source": [
    "## Etapa 6: Construir nosso método de avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d31eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actual_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "            actual_labels.extend(labels.cpu().tolist())\n",
    "    return accuracy_score(actual_labels, predictions), classification_report(actual_labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4ca134",
   "metadata": {},
   "source": [
    "## Etapa 7: Defina os parâmetros do nosso modelo\n",
    "\n",
    "Aqui, vamos configurar parâmetros essenciais para ajustar o BERTClassifier, incluindo o nome do modelo BERT, número de classes, comprimento máximo da sequência de entrada, tamanho do lote, número de períodos de treinamento e taxa de aprendizado, para ajudar o modelo a entender efetivamente as críticas de filmes e seus sentimentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66ecc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_name = \"google-bert/bert-base-uncased\"\n",
    "num_classes = 2\n",
    "max_length = 64\n",
    "batch_size = 8     \n",
    "num_epochs = 4\n",
    "learning_rate = 2e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f504c1",
   "metadata": {},
   "source": [
    "## Etapa 8: Carregando e dividindo os dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c34be2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6f7d31",
   "metadata": {},
   "source": [
    "## Etapa 9: inicializar o tokenizador, o conjunto de dados e o carregador de dados\n",
    "\n",
    "Para Windows na classe DataLoader o num_workers=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5efdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "train_dataset = TextClassificationDataset(train_texts, train_labels, tokenizer, max_length)\n",
    "val_dataset = TextClassificationDataset(val_texts, val_labels, tokenizer, max_length)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=0, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b711d81d",
   "metadata": {},
   "source": [
    "## Etapa 10: Configurar o dispositivo e o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54618e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0818b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BERTClassifier(bert_model_name, num_classes).to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc018de",
   "metadata": {},
   "source": [
    "## Etapa 11: Configurar o otimizador e o planejador de taxa de aprendizagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81c8bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "total_steps = len(train_dataloader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8046f684",
   "metadata": {},
   "source": [
    "## Etapa 12: Treinando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c85d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    train(model, train_dataloader, optimizer, scheduler, device)\n",
    "    accuracy, report = evaluate(model, val_dataloader, device)\n",
    "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d94991",
   "metadata": {},
   "source": [
    "Salvando o modelo final\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2339956b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"../model\", exist_ok=True)\n",
    "torch.save(model.state_dict(), \"../model/bert_classifier.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a5837d",
   "metadata": {},
   "source": [
    "## Etapa 13: Construa nosso método de previsão\n",
    "\n",
    "A função predict_sentiment() atua como nosso método de avaliação. Para cada lote, ela obtém os IDs de entrada, máscaras de atenção e rótulos e os alimenta para o modelo. O modelo então fornece suas melhores previsões, que são comparadas aos rótulos reais.\n",
    "\n",
    "Por fim, a função calcula a pontuação de precisão e um relatório de classificação para nos informar o quão bem o modelo se saiu na compreensão dos sentimentos das críticas de filmes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9dec5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text, model, tokenizer, device, max_length=128):\n",
    "    model.eval()\n",
    "    encoding = tokenizer(text, return_tensors='pt', max_length=max_length, padding='max_length', truncation=True)\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        \n",
    "    return \"positive\" if preds.item() == 1 else \"negative\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff50fd1f",
   "metadata": {},
   "source": [
    "## Etapa 14: Avaliando o desempenho do nosso modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63559882",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"The movie is very good and I recommend it to everyone.\"\n",
    "sentiment = predict_sentiment(test_text, model, tokenizer, device)\n",
    "print(\"The movie is very good and I recommend it to everyone.\")\n",
    "print(f\"Predicted sentiment: {sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7f0856",
   "metadata": {},
   "source": [
    "Resultado: O filme é muito bom e eu recomendaria a todos.\n",
    "\n",
    "Sentimento previsto: positivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9dfd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"Worst movie of the year.\"\n",
    "sentiment = predict_sentiment(test_text, model, tokenizer, device)\n",
    "print(\"Worst movie of the year.\")\n",
    "print(f\"Predicted sentiment: {sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9771abba",
   "metadata": {},
   "source": [
    "Resultado: Pior filme do ano.\n",
    "\n",
    "Sentimento previsto: negativo\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c43a190",
   "metadata": {},
   "source": [
    "## Considerações finais sobre a classificação de texto usando BERT\n",
    "\n",
    "Para resumir, o BERT mudou seriamente o jogo quando se trata de classificação de texto. Ele tornou coisas como análise de sentimentos e classificação de tópicos muito melhores e mais rápidas. Ao pegar esses modelos pré-treinados e personalizá-los para nossos próprios projetos, estamos obtendo resultados incríveis que nos ajudam no mundo real.\n",
    "\n",
    "À medida que continuamos experimentando o BERT e modelos similares, não há dúvidas de que veremos ainda mais coisas legais acontecendo no mundo da IA ​​e da compreensão de linguagem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
